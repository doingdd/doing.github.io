---
title: interview -- 京东五面
layout: post
category: life
---
## 1. 算法，数组，在连续数组中找出缺失的那一个数字。
给定一个包含1000个元素的数组，其中的元素是从1到1001的数字，但是缺失了一个数字，要求找到这个数字是什么。

首先给出的答案是假设数组有序的情况下：
```python
#!/usr/bin/python

def f(A):
    for i,v in enumerate(A):
        if v != i + 1:
            return i + 1
A=[i for i in range(1,1002)]
A.pop(364)
print f(A)
```
然后被要求考虑数组无序的情况，这里考虑了半天，思路是使用字典记录每个数字和它出现的情况，刚开始考虑的比较复杂，经过提示得出key记录数字本身，然后value记录数字出现的次数的思路，在遍历完成后，找到那个value不等于1的key，就是丢失的数字。  

还有一种办法，经过提示，使用求和公式，如果连续的数组求和:sum=(1+1001)*1001/2，然后和这个数组的真实和相减，得到的差值就是丢失的那个数字。

## 2. 开放性问题，101个硬币，真币重还是假币重。

101个硬币，里面有100个真的和1个假的，真假币仅重量不同，通过一个天秤，最少多少次可以得到真币重还是假币重的结论。
1. 正常思路是二分法，首先拿两个硬币称量，如果一轻一重比较好办，证明肯定有假币被拿出来了，再从堆里拿一个就可以比较了。  
如果一样重，则放回一个硬币，保证堆里面有100个硬币，这时假币也在里面，将其分成两份，一份50个，称量之后，先把重的50个分成25个的两份称一次如果25个一份的硬币一样重，证明假币在轻的那50个硬币里，也就是假币轻；如果25个一份的硬币一轻一重，那假币就在当前这个重的50个应比例，假币重。  

2.经提示，有三分法的思路，然后得出的办法是：先找出两个真币从堆里剔除，方法和上面提到的一样。然后剩下的99个硬币三等分，分别为a,b,c；如果a>b and a>c证明a里面有假币，假币重；如果a<b and a<c，则a里面有假币，假币轻；注意，不可能出现a>b and a<c的情况，因为如果出现这种情况，a,b,c的重量都不相等，是不可能只有一个假币的。  

3.还有一种三分的办法，分成33,33,35三份，33和33先比，如果相等则假币在35的那一份里，从这两份33的真币中拿出35个和其相比，则得出结论；如果33和33不等，则把35的那一份拿掉两个与原33的两份分别相比，谁和原35个的这份重量不等，谁有假币，得出结论。



## 3. 项目应用，统计文件夹下ascii文本文件中的英文单词的个数。
题目要求：某个文件夹内有大量的ascii格式的纯文本文件，统计出文件中每个英文单词出现的个数总和。

首先想到的使用shell实现：
```shell
root@Doing:~# cat test 
hello, my name is haha.
what's  your name?
hello, my name is heihei.
root@Doing:~# grep -oE '[[:alpha:]]*' test|sort|uniq -c
      1 haha
      1 heihei
      2 hello
      2 is
      2 my
      3 name
      1 s
      1 what
      1 your
```

然后又用python实现了一次：
```python
#!/usr/bin/python

def count(file):
    d = {}
    with open(file) as f:
        for line in f:
            for word in line.split():
                d[word] = d.get(word,0) + 1
    return d

print count('./test')

root@Doing:~/Jingdong# ./1.py 
{'hello,': 2, 'name': 2, "what's": 1, 'your': 1, 'haha.': 1, 'heihei.': 1, 'name?': 1, 'my': 2, 'is': 2}
```
然后就着这个程序分析，提出了很多问题：

1. 首字母大小写没有处理
2. 特殊符号没处理
3. 标点没处理
4. 性能方面，如果文件特别多，怎么提高效率？ 答了使用多线程，每个线程负责一定数量的文件。
5. 如果考虑有某个单个文件特别大，怎么办？答了使用一个size的阈值，累积超过这个阈值之后，交给下个线程，不过可能的问题是单个文件不能被拆分仍然导致单个大文件会一次性超过阈值非常大，然后提出了可以把大文件拆分的办法。  
6. 这个写法，需不需要考虑字典的key的溢出？答：不考虑，英文单词一共没多少个。
7. 需不需要考虑字典的value的溢出？答：不考虑，python中会自动将溢出最大值的整形进行分割处理，但是c和c++需要考虑。

## 4. 项目实践：从一台服务器上给100台服务器同步一个150g的文件，怎么做(效率最高)？

在copy完成之后，每台server会重启，需要保证在线更新，不能全部同事down掉。

1. 首先确定几个相关指标：单台对单台的copy速率；copy速率随着并发copy个数的增加的变化曲线；最多允许多少来server down掉服务不受影响；根据这些指标得出一个数字：并行copy的server个数。

从思路上，想到了分层模型，第一层就是第一台lab，同时给若干个server拷贝，然后它的子server每一台又有若干个子server，这样每台lab会负责检测自己是不是被更新了，然后决定是否copy给它的子server。   
但是这样有一个问题，如果其中某个中间节点down掉坏了，那么它的子server将不能接收到copy，这个需要引入更复杂的处理机制，比如每个server不仅仅handle一组子server。 

## 5. linux，head的实现机制
这个从来没研究过，尝试的解释了一下，定义一个count，每读入一行count加1并且输出当前行，到达指定值时停止读入。  
个人理解问题的核心是head究竟是不是完全读入了所有行，还是仅读入并输出指定数目的行。  

## 6. top命令打开，解释每一行的意思
参考[top 命令](http://doing.cool/2017/12/27/Linux-Performance-top.html) 

## 7. 算法：给定两个若干维的数组，元素全为数字，返回每个不一样的元素的index。  
输入[[1,2],3],[[2,1],3]，返回：[1,[0,0],[0,1]]， 1代表不相等，0代表相等，[0,0]是第0个元素中的第0个元素,[0,1]是第0个元素的第一个元素。

